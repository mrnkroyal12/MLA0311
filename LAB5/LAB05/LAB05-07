import numpy as np
import random
GRID_SIZE = 5
FOOD = (4, 4)
GHOST = (2, 2)
START = (0, 0)
ACTIONS = ['U', 'D', 'L', 'R']
ACTION_MAP = {
    'U': (-1, 0),
    'D': (1, 0),
    'L': (0, -1),
    'R': (0, 1)
}
Q = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))

# Hyperparameters
alpha = 0.1        
gamma = 0.9        
epsilon = 0.2      
episodes = 1000

def is_valid(x, y):
    return 0 <= x < GRID_SIZE and 0 <= y < GRID_SIZE

def reward(state):
    if state == FOOD:
        return 10
    elif state == GHOST:
        return -10
    else:
        return -1
for ep in range(episodes):
    state = START

    while state != FOOD and state != GHOST:
        x, y = state

        # Îµ-greedy action selection
        if random.uniform(0, 1) < epsilon:
            action_idx = random.randint(0, 3)
        else:
            action_idx = np.argmax(Q[x, y])

        action = ACTIONS[action_idx]
        dx, dy = ACTION_MAP[action]
        nx, ny = x + dx, y + dy

        if not is_valid(nx, ny):
            nx, ny = x, y

        next_state = (nx, ny)
        r = reward(next_state)

      
        Q[x, y, action_idx] += alpha * (
            r + gamma * np.max(Q[nx, ny]) - Q[x, y, action_idx]
        )

        state = next_state

policy = np.full((GRID_SIZE, GRID_SIZE), ' ')
for i in range(GRID_SIZE):
    for j in range(GRID_SIZE):
        if (i, j) == FOOD:
            policy[i][j] = 'F'
        elif (i, j) == GHOST:
            policy[i][j] = 'G'
        else:
            policy[i][j] = ACTIONS[np.argmax(Q[i, j])]

print("Learned Policy:")
print(policy)
